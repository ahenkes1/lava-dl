{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8onwPO-WvSvG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# XOR: A minimalistic regression tutorial for LAVA-DL\n",
        "by [Alexander Henkes](https://orcid.org/0000-0003-4615-9271)\n",
        "---\n",
        "\n",
        "In this tutorial we want to solve a simple regression task using [spiking neural\n",
        "networks](https://en.wikipedia.org/wiki/Spiking_neural_network) (SNNs) and the [**LAVA-DL** library](https://github.com/lava-nc/lava-dl). The presented approach tries to stay as\n",
        "minimalistic as possible, though it is easy to expand it to much more complex\n",
        "problems. A basic understanding of spiking neural networks is asumed. If you start from zero, take a look at [this tutorials](https://snntorch.readthedocs.io/en/latest/tutorials/index.html) first.\n"
      ],
      "metadata": {
        "id": "msLwzgSMpGqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation\n",
        "> If you are using Google Colab, uncomment the following cell to install all the\n",
        "> necessary packages! You can safely ignore all GPU related erorrs and warnings, we only need a CPU!"
      ],
      "metadata": {
        "id": "gwxrUmpzsIhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install the 'lava-nc' base package:\n",
        "!pip install --upgrade pip\n",
        "!pip install lava-nc\n",
        "\n",
        "# Second, install the 'lava-dl' deep learning addition, available from github:\n",
        "!rm ./lava*\n",
        "!curl -s https://api.github.com/repos/lava-nc/lava-dl/releases/latest \\\n",
        "| grep browser_download_url \\\n",
        "| cut -d '\"' -f 4 \\\n",
        "| grep tar.gz \\\n",
        "| wget -i -\n",
        "!pip install ./lava_dl*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NjqpuO0Fmfdx",
        "outputId": "e2fa8974-362f-45ab-8a75-6bd9ae410e95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.2.1\n",
            "Collecting lava-nc\n",
            "  Obtaining dependency information for lava-nc from https://files.pythonhosted.org/packages/29/39/607eafb3e98935ce1f195404081f2a11f55f72f6f08717b2899c742025e6/lava_nc-0.8.0-py3-none-any.whl.metadata\n",
            "  Downloading lava_nc-0.8.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting asteval<0.10.0,>=0.9.31 (from lava-nc)\n",
            "  Obtaining dependency information for asteval<0.10.0,>=0.9.31 from https://files.pythonhosted.org/packages/05/34/bdb51767967cb29302ee7dfe95662b057af7f23c62dd1967fc4b373656aa/asteval-0.9.31-py3-none-any.whl.metadata\n",
            "  Downloading asteval-0.9.31-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting networkx<=2.8.7 (from lava-nc)\n",
            "  Downloading networkx-2.8.7-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2.0.0,>=1.24.4 (from lava-nc)\n",
            "  Obtaining dependency information for numpy<2.0.0,>=1.24.4 from https://files.pythonhosted.org/packages/71/3c/3b1981c6a1986adc9ee7db760c0c34ea5b14ac3da9ecfcf1ea2a4ec6c398/numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from lava-nc) (1.10.1)\n",
            "Downloading lava_nc-0.8.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asteval-0.9.31-py3-none-any.whl (20 kB)\n",
            "Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, networkx, asteval, lava-nc\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asteval-0.9.31 lava-nc-0.8.0 networkx-2.8.7 numpy-1.25.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove './lava*': No such file or directory\n",
            "--2023-08-21 11:39:12--  https://github.com/lava-nc/lava-dl/releases/download/v0.4.0/lava_dl-0.4.0.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/411730917/1a095795-5494-4a6a-80a2-85b8ac1b5ee7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230821%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230821T113912Z&X-Amz-Expires=300&X-Amz-Signature=4de76a0b596e27cb969337a7c03cde510e8ce1eb26a2e015b2dc0f8d2436d52e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=411730917&response-content-disposition=attachment%3B%20filename%3Dlava_dl-0.4.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-08-21 11:39:12--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/411730917/1a095795-5494-4a6a-80a2-85b8ac1b5ee7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230821%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230821T113912Z&X-Amz-Expires=300&X-Amz-Signature=4de76a0b596e27cb969337a7c03cde510e8ce1eb26a2e015b2dc0f8d2436d52e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=411730917&response-content-disposition=attachment%3B%20filename%3Dlava_dl-0.4.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41620805 (40M) [application/octet-stream]\n",
            "Saving to: ‘lava_dl-0.4.0.tar.gz’\n",
            "\n",
            "lava_dl-0.4.0.tar.g 100%[===================>]  39.69M  58.5MB/s    in 0.7s    \n",
            "\n",
            "2023-08-21 11:39:13 (58.5 MB/s) - ‘lava_dl-0.4.0.tar.gz’ saved [41620805/41620805]\n",
            "\n",
            "FINISHED --2023-08-21 11:39:13--\n",
            "Total wall clock time: 1.5s\n",
            "Downloaded: 1 files, 40M in 0.7s (58.5 MB/s)\n",
            "Processing ./lava_dl-0.4.0.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: h5py<4.0.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from lava-dl==0.4.0) (3.9.0)\n",
            "Requirement already satisfied: lava-nc==0.8.0 in /usr/local/lib/python3.10/dist-packages (from lava-dl==0.4.0) (0.8.0)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.5.2 in /usr/local/lib/python3.10/dist-packages (from lava-dl==0.4.0) (3.7.1)\n",
            "Collecting ninja<2.0.0.0,>=1.10.2.3 (from lava-dl==0.4.0)\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from lava-dl==0.4.0) (1.25.2)\n",
            "Requirement already satisfied: pillow<10.0.0,>=9.2.0 in /usr/local/lib/python3.10/dist-packages (from lava-dl==0.4.0) (9.4.0)\n",
            "Requirement already satisfied: pytest<8.0.0,>=7.2.0 in /usr/local/lib/python3.10/dist-packages (from lava-dl==0.4.0) (7.4.0)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from lava-dl==0.4.0) (1.10.1)\n",
            "Collecting torch<2.0.0,>=1.13.1 (from lava-dl==0.4.0)\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision<0.15.0,>=0.14.0 (from lava-dl==0.4.0)\n",
            "  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unittest2<2.0.0,>=1.1.0 (from lava-dl==0.4.0)\n",
            "  Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: asteval<0.10.0,>=0.9.31 in /usr/local/lib/python3.10/dist-packages (from lava-nc==0.8.0->lava-dl==0.4.0) (0.9.31)\n",
            "Requirement already satisfied: networkx<=2.8.7 in /usr/local/lib/python3.10/dist-packages (from lava-nc==0.8.0->lava-dl==0.4.0) (2.8.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.5.2->lava-dl==0.4.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.5.2->lava-dl==0.4.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.5.2->lava-dl==0.4.0) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.5.2->lava-dl==0.4.0) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.5.2->lava-dl==0.4.0) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.5.2->lava-dl==0.4.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.5.2->lava-dl==0.4.0) (2.8.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest<8.0.0,>=7.2.0->lava-dl==0.4.0) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest<8.0.0,>=7.2.0->lava-dl==0.4.0) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest<8.0.0,>=7.2.0->lava-dl==0.4.0) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest<8.0.0,>=7.2.0->lava-dl==0.4.0) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.0.0,>=1.13.1->lava-dl==0.4.0) (4.7.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2.0.0,>=1.13.1->lava-dl==0.4.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2.0.0,>=1.13.1->lava-dl==0.4.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<2.0.0,>=1.13.1->lava-dl==0.4.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2.0.0,>=1.13.1->lava-dl==0.4.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0,>=1.13.1->lava-dl==0.4.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0,>=1.13.1->lava-dl==0.4.0) (0.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision<0.15.0,>=0.14.0->lava-dl==0.4.0) (2.31.0)\n",
            "Collecting argparse (from unittest2<2.0.0,>=1.1.0->lava-dl==0.4.0)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: six>=1.4 in /usr/local/lib/python3.10/dist-packages (from unittest2<2.0.0,>=1.1.0->lava-dl==0.4.0) (1.16.0)\n",
            "Collecting traceback2 (from unittest2<2.0.0,>=1.1.0->lava-dl==0.4.0)\n",
            "  Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision<0.15.0,>=0.14.0->lava-dl==0.4.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision<0.15.0,>=0.14.0->lava-dl==0.4.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision<0.15.0,>=0.14.0->lava-dl==0.4.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision<0.15.0,>=0.14.0->lava-dl==0.4.0) (2023.7.22)\n",
            "Collecting linecache2 (from traceback2->unittest2<2.0.0,>=1.1.0->lava-dl==0.4.0)\n",
            "  Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: lava-dl\n",
            "  Building wheel for lava-dl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lava-dl: filename=lava_dl-0.4.0-py3-none-any.whl size=5535446 sha256=b9f6a0c4ed2f973b200e0812c63e521a1dc454e2ee42478d5a1a926fd1fe646b\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/db/bb/936869216d10bcba3e77b09022a8a301d5cec6a18ef38161c1\n",
            "Successfully built lava-dl\n",
            "Installing collected packages: ninja, linecache2, argparse, traceback2, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, unittest2, nvidia-cudnn-cu11, torch, torchvision, lava-dl\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed argparse-1.4.0 lava-dl-0.4.0 linecache2-1.0.0 ninja-1.11.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1 torchvision-0.14.1 traceback2-1.4.0 unittest2-1.1.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and random seeds"
      ],
      "metadata": {
        "id": "wmEmgbWivHT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Solve XOR with LIFs using LAVA/SLAYER.\"\"\"\n",
        "import lava.lib.dl.slayer as slayer\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "SEED = 666\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbo4p4EKtlxt",
        "outputId": "2048b004-0235-4674-bede-b6758463750f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb3a864e6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression\n",
        "[Regression](https://en.wikipedia.org/wiki/Regression_analysis) it the task of relating some ($n_x$-dimensional, real-valued) input to some ($n_y$-dimensional, real-valued) output, such that\n",
        "\n",
        "\\begin{align}\n",
        "    f: \\mathbb{R}^{n_x} &\\to \\mathbb{R}^{n_y} \\\\\n",
        "    \\mathbf{x} &\\mapsto \\mathbf{y}.\n",
        "\\end{align}\n",
        "\n",
        "This simple statement can describe all kinds of nonlinear functions whith possibly very complicated behavior. If we do not know an analytical expression for our complicated function $f$ but have access to some input data $\\mathbf{x}$ and output data $\\mathbf{y}$, we can approximate it using a neural network $\\mathcal{N}$, such that\n",
        "\n",
        "\\begin{equation}\n",
        "    f \\approx \\mathcal{N}.\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "mtaGSyb7puyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XOR\n",
        "Here, we take a look at the [XOR problem](https://en.wikipedia.org/wiki/Exclusive_or), which relates a two-dimensional input pair to a one-dimensional output\n",
        "\n",
        "\\begin{align}\n",
        "    \\operatorname{XOR}: \\mathbb{R} \\times \\mathbb{R} &\\to \\mathbb{R} \\\\\n",
        "    (A, B) &\\mapsto A \\oplus B\n",
        "\\end{align}\n",
        "\n",
        "It can be described by the following table:\n",
        "\n",
        "\\begin{array}{c}\n",
        "A & B & A \\oplus B \\\\ \\hline\n",
        "0 & 0 & 0 \\\\\n",
        "0 & 1 & 1 \\\\\n",
        "1 & 0 & 1 \\\\\n",
        "1 & 1 & 0 \\\\ \\hline\n",
        "\\end{array}\n",
        "\n",
        "We see that our dataset is indeed quite minimalistic, it merely consists of four samples. We can define the dataset using the [standard PyTorch approach](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files). SNNs are inherently time-dependent, but the XOR dataset is static. We simply generate a pseudo-time axis by repeating every sample for as many time steps as we like."
      ],
      "metadata": {
        "id": "VZkX-6tBucs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XOR(torch.utils.data.Dataset):\n",
        "    \"\"\"XOR dataset.\n",
        "\n",
        "    Produce a torch.dataset for the XOR problem. It consists of two inputs\n",
        "    and one output correspoinding to the following logic table:\n",
        "\n",
        "    Input   |   Output\n",
        "    ==================\n",
        "    (0, 0)  |   (0)\n",
        "    (0, 1)  |   (1)\n",
        "    (1, 0)  |   (1)\n",
        "    (1, 1)  |   (0)\n",
        "    ==================\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, time):\n",
        "        \"\"\"Initialize dataset.\n",
        "\n",
        "        The dataset consists of two-dimensional input features and\n",
        "        one-dimensional output labels. The axis convention\n",
        "\n",
        "            (BATCH, TIME, FEATURE)\n",
        "\n",
        "        is used. The parameter 'time' controlls the number of\n",
        "        discrete pseudo-time steps.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        time : int\n",
        "            Number of discrete time steps needed for LIF-type neurons.\n",
        "\n",
        "        \"\"\"\n",
        "        self.feature = torch.Tensor(\n",
        "            [\n",
        "                [0, 0],\n",
        "                [0, 1],\n",
        "                [1, 0],\n",
        "                [1, 1],\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.label = torch.Tensor(\n",
        "            [\n",
        "                [0],\n",
        "                [1],\n",
        "                [1],\n",
        "                [0],\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.feature = torch.unsqueeze(self.feature, -1)\n",
        "        self.label = torch.unsqueeze(self.label, -1)\n",
        "\n",
        "        self.feature = torch.repeat_interleave(\n",
        "            input=self.feature, repeats=time, dim=-1\n",
        "        )\n",
        "        self.label = torch.repeat_interleave(\n",
        "            input=self.label, repeats=time, dim=-1\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return length of dataset.\n",
        "\n",
        "        The length of the dataset is defined as the length of the\n",
        "        first axis, the batch axis.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            Number of unique samples in the dataset.\n",
        "\n",
        "        \"\"\"\n",
        "        return len(self.feature[:, 0, 0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return a single sample from the dataset.\n",
        "\n",
        "        Return a single sample from the dataset using the index variable 'idx'.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            Index of the sample.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Sample 'idx' from the dataset.\n",
        "\n",
        "        \"\"\"\n",
        "        return self.feature[idx, :, :], self.label[idx, :, :]\n"
      ],
      "metadata": {
        "id": "nCDtNSJ4tvaE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural network\n",
        "\n",
        "In this tutorial we want to use SNNs to fit real-valued XOR problem. You can solve this as a classification task, probably using spikes directly, but we chose a different approach which translates well to all kinds of regression scenarios.\n",
        "\n",
        "The problem in regression with SNNs lies in the binary (or unary, if you like) nature of information travel between neurons. On the one hand, this leads to temporal- and inter-spike sparsity and therefore, to massive energy savings on neuromorphic hardware, on the other hand representing real-valued functions with spikes is not straightforward.\n",
        "\n",
        "First, we need to convert our real-valued input to spikes using some sort of encoder. In this tutorial, we will use a simple CUBA, a second-order variant of the classical [LIF](https://neuronaldynamics.epfl.ch/online/Ch1.S3.html) with richer neural dynamics. The encoder simply adds the input values constantly over time to a CUBA neuron, which generates spikes\n",
        "\n",
        "\\begin{align}\n",
        "    \\operatorname{encoder}: \\mathbb{R} \\times \\mathbb{R} &\\to \\{0, 1\\}^t.\n",
        "\\end{align}\n",
        "\n",
        "In **LAVA-DL**, this is realized by the `slayer.block.cuba.Input` layer, which organises several neurons in a stack. In between we can use as many spiking layers as we want\n",
        "\n",
        "\\begin{align}\n",
        "    \\operatorname{spiking neuron}: \\{0, 1\\}^t &\\to \\{0, 1\\}^t.\n",
        "\\end{align}\n",
        "\n",
        "Here, we chose the `slayer.block.cuba.Dense` layer which combines a dense feed-forward neural network with CUBA dynamics. For the output we take the membrane potential of the neuron in the last layers. For more details for this approach you can take a look at [https://arxiv.org/abs/2210.03515](https://arxiv.org/abs/2210.03515). The potential is a real-valued number and can be extracted via the `slayer.block.cuba.Affine` layer. It acts as some sort of decoder, from spikes to real-valued numbers\n",
        "\n",
        "\\begin{align}\n",
        "    \\operatorname{decoder}: \\{0, 1\\}^t &\\to \\mathbb{R}.\n",
        "\\end{align}\n",
        "\n",
        "Again, we can define the network using [PyTorch](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html). For details on the nasty details, see the [**LAVA-DL** documentation](https://lava-nc.org/lava-lib-dl/index.html)."
      ],
      "metadata": {
        "id": "8onwPO-WvSvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(torch.nn.Module):\n",
        "    \"\"\"LIF network.\n",
        "\n",
        "    A network consisting of the following topology:\n",
        "\n",
        "    Layer\n",
        "    ===============\n",
        "    - BlockCubaInput\n",
        "    - BlockCubaDense\n",
        "    - BlockCubaDense\n",
        "    - BlockCubaAffine\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize network.\"\"\"\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        cuba_params = {\n",
        "            \"threshold\": 0.1,\n",
        "            \"current_decay\": 0.9,\n",
        "            \"voltage_decay\": 0.9,\n",
        "            \"tau_grad\": 1,\n",
        "            \"scale_grad\": 1,\n",
        "            \"scale\": 1 << 6,\n",
        "            \"norm\": None,\n",
        "            \"dropout\": None,\n",
        "            \"shared_param\": True,\n",
        "            \"persistent_state\": False,\n",
        "            \"requires_grad\": False,\n",
        "            \"graded_spike\": False,\n",
        "        }\n",
        "\n",
        "        width = 32\n",
        "\n",
        "        self.blocks = torch.nn.ModuleList(\n",
        "            [\n",
        "                slayer.block.cuba.Input(\n",
        "                    neuron_params=cuba_params, count_log=False\n",
        "                ),\n",
        "                slayer.block.cuba.Dense(\n",
        "                    neuron_params=cuba_params,\n",
        "                    in_neurons=2,\n",
        "                    out_neurons=width,\n",
        "                    count_log=False,\n",
        "                ),\n",
        "                slayer.block.cuba.Dense(\n",
        "                    neuron_params=cuba_params,\n",
        "                    in_neurons=width,\n",
        "                    out_neurons=width,\n",
        "                    count_log=False,\n",
        "                ),\n",
        "                slayer.block.cuba.Affine(\n",
        "                    neuron_params=cuba_params,\n",
        "                    in_neurons=width,\n",
        "                    out_neurons=1,\n",
        "                    dynamics=False,\n",
        "                    count_log=False,\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        count = []\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "            count.append(torch.mean(x).item())\n",
        "\n",
        "        return x, torch.as_tensor(count)\n"
      ],
      "metadata": {
        "id": "dTFsXv7rt05a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop\n",
        "\n",
        "The training using [**LAVA-DL**](https://lava-nc.org/dl.html#lava-dl-workflow) is carried out like a PyTorch training loop, but some details are handled via the library directly. First, we chose an [optimizer](https://pytorch.org/docs/stable/optim.html) and define a [dataloader](https://pytorch.org/docs/stable/optim.html). The training itself including logging is carried out by `slayer.utils.Assistant()` in conjunction with `slayer.utils.LearningStats()`. With the help of a `lambda` function we define a simple mean-squared error loss on the last time step of our pseudo-time (remember: SNNs are inherently time-dependent, therefore we introduced a pseudo-time in our static data to be able to make use of the neuron dynamics). Finally, we loop over our training set. Additionally, we track the number of spikes produced by every layer. This gives us information about the level of sparsity of our network."
      ],
      "metadata": {
        "id": "IBjlXUFIvWD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, dataset, epochs):\n",
        "    \"\"\"Train the network.\"\"\"\n",
        "    optimizer = torch.optim.AdamW(net.parameters(), lr=1e-3)\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=dataset, batch_size=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    stats = slayer.utils.LearningStats(\n",
        "        loss_str=\"loss\",\n",
        "        loss_unit=\"\",\n",
        "        accuracy_str=\"\",\n",
        "        accuracy_unit=\"\",\n",
        "    )\n",
        "\n",
        "    assistant = slayer.utils.Assistant(\n",
        "        net=net,\n",
        "        error=lambda output, target: torch.nn.functional.mse_loss(\n",
        "            output[:, :, -1].flatten(), target[:, :, -1].flatten()\n",
        "        ),\n",
        "        optimizer=optimizer,\n",
        "        stats=stats,\n",
        "        classifier=None,\n",
        "        count_log=True,\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, (feature, label) in enumerate(dataloader):\n",
        "            _, count = assistant.train(feature, label)\n",
        "            print(f\"\\r[Epoch {epoch:3d}/{epochs}] {stats}\", end=\"\")\n",
        "\n",
        "        stats.update()\n",
        "\n",
        "    return stats, count\n"
      ],
      "metadata": {
        "id": "5YGkNWVkt6Q2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main function\n",
        "\n",
        "In the main function we go through the following steps:\n",
        "\n",
        "\n",
        "1.   Create the SNN `net`\n",
        "2.   Create the XOR-dataset `dataset` with pseudo-time `time`\n",
        "3.   Train the network using `train()`\n",
        "4.   Print sparsity information `spike_activity`\n",
        "5.   Predict `prediction` and print some results!\n",
        "\n"
      ],
      "metadata": {
        "id": "T1T9Zi4CvYn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Execute main function.\"\"\"\n",
        "    net = Network()\n",
        "    dataset = XOR(time=10)\n",
        "\n",
        "    _, count = train(net=net, dataset=dataset, epochs=500)\n",
        "\n",
        "    spike_activity = [str(round(i.item() * 100, 2)) for i in count.numpy()]\n",
        "    spike_activity = \"| \" + \"\".join(x + \"% | \" for x in spike_activity)\n",
        "    print(f\"\\n\\nSpike activity per layer: {spike_activity}\\n\")\n",
        "\n",
        "    prediction = torch.round(net(dataset.feature)[0])\n",
        "\n",
        "    print(\n",
        "        f\"{'Input:':<12}{dataset.feature[:, :, -1].detach().numpy().tolist()}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"{'Output:':<12}{dataset.label[:, :, -1].detach().numpy().tolist()}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"{'Prediction:':<12}{prediction[:, :, -1].detach().numpy().tolist()}\"\n",
        "    )\n",
        "\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "vukYGg5RtO8O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run it!\n",
        "\n",
        "You can run everything (again and again) using the following cell. The layout of the notebook was chosen in order to get a nice `.py` script when exporting. It can aid as a solid basis for your own experiments!"
      ],
      "metadata": {
        "id": "4pFzUkHeva37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6hdAZTkt96z",
        "outputId": "6793a6a8-8b85-44aa-b357-29a2c94beb91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 499/500] Train loss =     0.00098 (min =     0.00000)      = 0.00000 (max = 0.00000) \n",
            "\n",
            "Spike activity per layer: | 45.0% | 26.09% | 18.28% | 35.78% | \n",
            "\n",
            "Input:      [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
            "Output:     [[0.0], [1.0], [1.0], [0.0]]\n",
            "Prediction: [[0.0], [1.0], [1.0], [0.0]]\n"
          ]
        }
      ]
    }
  ]
}